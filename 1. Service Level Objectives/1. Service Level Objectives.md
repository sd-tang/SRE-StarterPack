# 1 Service Level Objectives

## 1.0 Understanding the System
Before we go into defining our SLIs, SLOs, and SLAs we want to have a high-level understanding of the system that we want to monitor. Start with a 'skeleton' and we can iterate from there.

### 1.0.0 Home Loans System
I shall begin with a high-level understanding of the systems used by our customers, Home Loan Specialists, and Credit Assessors.

<br />
    <p align=center>
        <img src="https://github.com/sd-tang/SRE-StarterPack/blob/master/1.%20Service%20Level%20Objectives/Resources/HomeLoansSystem.png" />
    </p>
<br />

Based on this high-level architecture we can now attempt to draft the Indicators.

## 1.1 Service Level Indicators
Given the above setup, we can start thinking about how user interact with the system, and what sort of Indicators would measure the various aspects of a user's experience. It is important to remember we want indicators of user's happiness and satisfaction towards the system they interact with; as opposed to a techie's outer-worldly urge and expectations.

### 1.1.0 SLI Specification
Here are some good stuff to start with:

#### 1.1.0.0 Website/Webapp
| No | Type of SLI | Description |
| --- | --- | --- |
| 1 | Request latency | How long it takes to return a response to a request |
| 2 | Error rate | Failed requests |
| 3 | Availability/Yield | Well-formed requests that succeed |

#### 1.1.0.1 Home Loan Assessment System
| No | Type of SLI | Description |
| --- | --- | --- |
| 1 | Request latency | How long it takes to return a response to a request |
| 2 | Error rate | Failed requests |
| 3 | Availability/Yield | Well-formed requests that succeed |
| 4 | System throughput | Capacity to handle requests |
| 5 | Data Durability | Likelihood that data will be retained over a long period of time |

### 1.1.1 SLI Implementation
Let's start with something that requires the least engineering work, something that we already have.

#### 1.1.1.0 Application Server Logs
Our application server logs should capture sufficient information for us to measure. For availability and error rate, we need success/failure status; for request latency, we need time taken to serve the requests.

#### 1.1.1.1 Load Balancer Monitoring
Next step we should strive to move towards load balancer monitoring as it can provide indicators closer to user's experience. In addition to those measurable by application server logs, we can now measure system throughput by collecting metrics on requests handling.

### 1.1.2 Measuring SLIs
Now let's try to measure some of the SLIs mentioned above. We will use the example of measuring availability and latency.

Requests returning Error 5xx
```csharp
    httpRequestsTotal{host="T3", status="5*"}
```

Total latency, as a cumulative histogram; each metric counts the number of requests that took less than or equal to that time:
```csharp
    httpRequestDurationInSeconds{host="T3", le="0.1"}
    httpRequestDurationInSeconds{host="T3", le="0.2"}
    httpRequestDurationInSeconds{host="T3", le="0.4"}
```

From this let's calculate our first SLIs, 

Total Non-5xx Requests over Total Requests:
```csharp
    SumOf(httpRequestsTotal{host="T3", status!~"5*"}[7d]) / 
    SumOf(httpRequestsTotal{host="T3"}[7d])
```

90th and 99th percentile latency:
```csharp
    histogramQuantile(0.9, rateOverNumberOfDays(httpRequestDurationInSeconds[7d]))
    histogramQuantile(0.99, rateOverNumberOfDays(httpRequestDurationInSeconds[7d]))
```

## 1.2 Service Level Objectives
### 1.2.0 Determining Our Starter SLOs
Now let's say we have measured the above SLIs over a 4-week period and received the following readings:
 * Total requests: 3,663,253
 * Total successful requests: 3,557,865 (97.123%)
 * 90th percentile latency: 432 ms
 * 99th percentile latency: 891ms

We can then come up with a proposed SLO.

| SLO Type | Objective |
| --- | --- |
| Availability | 97% |
| Latency | 90% of requests < 450 ms |
| Latency | 99% of requests < 900 ms |

### 1.2.1 Error Budget
I would like to introduce a simple implementation of error budget here. We should however strive to adopt the SLO culture before selling the adoption of error budget culture.

| SLO | Allowed Failures |
| --- | --- |
| 97% availability | 109,897 |
| 90% of requests faster than 450 ms | 366,325 |
| 99% of requests faster than 900ms | 36,632 |

This Error Budget culture requires a more in-depth case to buy stakeholder's agreement to implement.
SLOs will no longer be a KPI but will be a trigger for action.

## 1.3 Action Plan
### 1.3.0 Decision Making Based On SLOs and Error Budget
This decision making matrix illustrates how we can take action based on our starter SLOs and error budget.

| SLO Target | Toil | Customer Satisfaction | Action |
| --- | --- | --- | --- |
| Met | Low | High | (a) Relax release and deployment processes, and increase velocity; or (b) Step back from the engagement and focus engineering time on services that need more reliability. |
| Met | Low | Low | Tighten SLO. |
| Met | High | High | If alerting is generating false positives, reduce sensitivity. Otherwise, temporarily loosen the SLOs (or offload toil) and fix product and/or improve automated fault mitigation. |
| Met | High | Low | Tighten SLO. |
| Missed | Low | High | Loosen SLO. |
| Missed | Low | Low | Increase alerting sensitivity. |
| Missed | High | High | Lossen SLO. |
| Missed | High | Low | Stop feature release. Both SRE and Dev teams to prioritise tasks that offload toil and fix product and/or improve automated fault mitigation. |

## 1.4 Conclusion
Coming up with Starter SLOs is foundational to begin our journey in adopting the SRE practice.

To summarize,
* SLOs are the tool to measure our service reliability
* Error budgets are a tool for balancing between reliability and velocity
* We can start using our Starter SLOs today and iterate from here